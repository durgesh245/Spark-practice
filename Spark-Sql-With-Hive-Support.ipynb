{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7dc5af",
   "metadata": {},
   "source": [
    "# Spark Sql with hive support without installation of Hive. \n",
    "### Here we have to configure warehouse.dir and spark will create metastore_db folder to store all the table and database details which supported by apache derby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3742c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n",
      "[('spark.sql.warehouse.dir', 'D:\\\\pythonWithPyspark\\\\Spark-practice\\\\spark-warehouse'), ('spark.driver.extraJavaOptions', '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'), ('spark.app.name', 'MySparkAppWithHiveSupport'), ('spark.app.startTime', '1659548633454'), ('spark.executor.id', 'driver'), ('spark.driver.port', '64574'), ('spark.app.submitTime', '1659548633228'), ('spark.sql.catalogImplementation', 'hive'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.driver.host', 'IMP-ITPL0888.impetus.co.in'), ('spark.app.id', 'local-1659548636023'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from os.path import abspath\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "abs_path = abspath('spark-warehouse')\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName(\"MySparkAppWithHiveSupport\")\\\n",
    ".config(\"spark.sql.warehouse.dir\", abs_path)\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext;\n",
    "print(sc.version)\n",
    "print(sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653d9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive;\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aadddee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE src;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5676529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"INSERT INTO src (key, value) values (1, 'D1'),(2, 'D2'),(3, 'D3');\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8255a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  1|   D1|\n",
      "|  2|   D2|\n",
      "|  3|   D3|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF1 = spark.sql(\"select * from src\")\n",
    "DF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97de23fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    namespace|\n",
      "+-------------+\n",
      "|customer_data|\n",
      "|      default|\n",
      "+-------------+\n",
      "\n",
      "+--------------+--------------------+\n",
      "|     info_name|          info_value|\n",
      "+--------------+--------------------+\n",
      "|Namespace Name|       customer_data|\n",
      "|       Comment|This is customer ...|\n",
      "|      Location|file:/D:/pythonWi...|\n",
      "|         Owner|      durgesh.pandey|\n",
      "|    Properties|                    |\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"DROP DATABASE customer_data;\")\n",
    "spark.sql(\"create database if not exists customer_data COMMENT 'This is customer database';\")\n",
    "spark.sql(\"show databases;\").show()\n",
    "spark.sql(\"describe database extended customer_data;\").show(10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4788e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      " |-- r_version: string (nullable = true)\n",
      " |-- r_arch: string (nullable = true)\n",
      " |-- r_os: string (nullable = true)\n",
      " |-- package: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- ip_id: integer (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|               date|               time|   size|r_version|r_arch|   r_os|  package|version|country|ip_id|\n",
      "+-------------------+-------------------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|2015-12-12 00:00:00|2022-08-04 13:42:10| 257886|    3.2.2|  i386|mingw32| HistData|  0.7-6|     CZ|    1|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:24:37|1236751|    3.2.2|x86_64|mingw32|  RJSONIO|  1.3-0|     DE|    2|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:42:35|2077876|    3.2.2|  i386|mingw32|   UsingR|  2.0-5|     CZ|    1|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:42:01| 266724|    3.2.2|  i386|mingw32|gridExtra|  2.0.0|     CZ|    1|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:00:21|3687766|       NA|    NA|     NA|     lme4| 1.1-10|     DE|    3|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:08:56|  57429|       NA|    NA|     NA| testthat| 0.11.0|     DE|    3|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:08:09| 216068|    3.2.2|x86_64|mingw32|  mvtnorm|  1.0-3|     DE|    4|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:25:00|3595497|    3.2.2|x86_64|mingw32|     maps|  3.0.1|     DE|    2|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:25:05|1579597|    3.2.2|x86_64|mingw32|       sp|  1.2-1|     DE|    2|\n",
      "|2015-12-12 00:00:00|2022-08-04 13:25:21| 892152|    3.2.3|x86_64|mingw32|geosphere|  1.4-3|     DE|    2|\n",
      "+-------------------+-------------------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use customer_data;\")\n",
    "path=\"sample_data/2015-12-12.csv\"\n",
    "csv_df=spark.read.options(inferSchema=True, header=True).csv(path)\n",
    "csv_df.printSchema()\n",
    "csv_df.show(10)\n",
    "csv_df.write.saveAsTable(\"cust_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eff6737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|               date|               time|   size|r_version|r_arch|   r_os|  package|version|country|ip_id|\n",
      "+-------------------+-------------------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|2015-12-12 00:00:00|2022-08-04 02:58:25|  69772|    3.2.2|x86_64|mingw32|  acepack|1.3-3.3|     KR|  139|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:58:25| 266727|    3.2.2|x86_64|mingw32|gridExtra|  2.0.0|     KR|  139|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:58:26|1630065|    3.2.2|x86_64|mingw32|    Hmisc| 3.17-0|     KR|  139|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:58:32|  14621|       NA|    NA|     NA|megaptera|  1.0-0|     CN|11333|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:59:01|3195824|    3.2.2|x86_64|mingw32|     Rcpp| 0.12.2|     CN|11334|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:59:05|1114434|    3.2.2|x86_64|mingw32|     plyr|  1.8.3|     CN|11334|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:59:12| 794788|       NA|    NA|     NA|     mgcv|  1.8-9|     BR| 1404|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:58:53| 164877|       NA|    NA|     NA| pbkrtest|  0.4-3|     HK| 4129|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:59:12|9494061|       NA|    NA|     NA| xlsxjars|  0.6.1|     US|11335|\n",
      "|2015-12-12 00:00:00|2022-08-04 02:58:54| 488081|       NA|    NA|     NA|     MASS| 7.3-45|     BR| 1404|\n",
      "+-------------------+-------------------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from cust_data where ip_id > 4\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a4e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
