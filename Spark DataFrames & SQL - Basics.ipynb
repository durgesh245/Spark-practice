{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames & SQL - Basics\n",
    "\n",
    "https://github.com/XD-DENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"My-DataFrame-Basics\").getOrCreate()\n",
    "sc = spark.sparkContext;\n",
    "sqlContext = spark\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Core in the machine is => 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1659507718957'),\n",
       " ('spark.app.submitTime', '1659507715431'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.app.name', 'My-DataFrame-Basics'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '59953'),\n",
       " ('spark.app.startTime', '1659507715747'),\n",
       " ('spark.driver.host', 'IMP-ITPL0888.impetus.co.in'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/D:/pythonWithPyspark/Spark-practice/spark-warehouse')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.default.parallelism\", \"1\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "print(f\"Total Core in the machine is => {sc.defaultParallelism}\")\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Build Spark DataFrames from Python Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "('A', 'B')\n",
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "|  5|  6|\n",
      "+---+---+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build a dataframe from Python Lists.\n",
    "\n",
    "#DT1 = sqlContext.createDataFrame(data=[(1,2), (3,4)], schema=(\"A\", \"B\"))\n",
    "#DT1.show()\n",
    "#df1 = spark.createDataFrame([(1, 3), (3, 4)])\n",
    "#df1.show()\n",
    "\n",
    "rd1 = sc.parallelize([('A', 'B'),(1,2), (3,4), (5,6)])\n",
    "print(type(rd1.collect()))\n",
    "print(rd1.filter(lambda x:x[0] == 'A').collect()[0])\n",
    "df2 = spark.createDataFrame(data=rd1.filter(lambda x:x[0] != 'A'), schema=rd1.filter(lambda x:x[0] == 'A').collect()[0])\n",
    "print(df2.show(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build Spark DataFrames from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"date\",\"time\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"',\n",
       " '\"2015-12-12\",\"13:42:10\",257886,\"3.2.2\",\"i386\",\"mingw32\",\"HistData\",\"0.7-6\",\"CZ\",1',\n",
       " '\"2015-12-12\",\"13:24:37\",1236751,\"3.2.2\",\"x86_64\",\"mingw32\",\"RJSONIO\",\"1.3-0\",\"DE\",2',\n",
       " '\"2015-12-12\",\"13:42:35\",2077876,\"3.2.2\",\"i386\",\"mingw32\",\"UsingR\",\"2.0-5\",\"CZ\",1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"sample_data/2015-12-12.csv\",\n",
    "            use_unicode=True).\\\n",
    "take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = sc.textFile(\"sample_data/2015-12-12.csv\", use_unicode=True) \\\n",
    "                    .map(lambda x:x.replace('\"', \"\")) \\\n",
    "                    .map(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['date',\n",
       "  'time',\n",
       "  'size',\n",
       "  'r_version',\n",
       "  'r_arch',\n",
       "  'r_os',\n",
       "  'package',\n",
       "  'version',\n",
       "  'country',\n",
       "  'ip_id'],\n",
       " ['2015-12-12',\n",
       "  '13:42:10',\n",
       "  '257886',\n",
       "  '3.2.2',\n",
       "  'i386',\n",
       "  'mingw32',\n",
       "  'HistData',\n",
       "  '0.7-6',\n",
       "  'CZ',\n",
       "  '1']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there can not be \".\" in the column names (header)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|      date|    time|   size|r_version|r_arch|   r_os|  package|version|country|ip_id|\n",
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|2015-12-12|13:42:10| 257886|    3.2.2|  i386|mingw32| HistData|  0.7-6|     CZ|    1|\n",
      "|2015-12-12|13:24:37|1236751|    3.2.2|x86_64|mingw32|  RJSONIO|  1.3-0|     DE|    2|\n",
      "|2015-12-12|13:42:35|2077876|    3.2.2|  i386|mingw32|   UsingR|  2.0-5|     CZ|    1|\n",
      "|2015-12-12|13:42:01| 266724|    3.2.2|  i386|mingw32|gridExtra|  2.0.0|     CZ|    1|\n",
      "|2015-12-12|13:00:21|3687766|       NA|    NA|     NA|     lme4| 1.1-10|     DE|    3|\n",
      "|2015-12-12|13:08:56|  57429|       NA|    NA|     NA| testthat| 0.11.0|     DE|    3|\n",
      "|2015-12-12|13:08:09| 216068|    3.2.2|x86_64|mingw32|  mvtnorm|  1.0-3|     DE|    4|\n",
      "|2015-12-12|13:25:00|3595497|    3.2.2|x86_64|mingw32|     maps|  3.0.1|     DE|    2|\n",
      "|2015-12-12|13:25:05|1579597|    3.2.2|x86_64|mingw32|       sp|  1.2-1|     DE|    2|\n",
      "|2015-12-12|13:25:21| 892152|    3.2.3|x86_64|mingw32|geosphere|  1.4-3|     DE|    2|\n",
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, time: string, size: string, r_version: string, r_arch: string, r_os: string, package: string, version: string, country: string, ip_id: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT2 = sqlContext.createDataFrame(data = dat.filter(lambda x:x[0]!='date'),\n",
    "                                 schema=dat.filter(lambda x:x[0]=='date').\\\n",
    "                                 collect()[0])\n",
    "DT2.show(n=10)\n",
    "DT2.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|      date|    time|   size|r_version|r_arch|   r_os|  package|version|country|ip_id|\n",
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|2015-12-12|13:42:10| 257886|    3.2.2|  i386|mingw32| HistData|  0.7-6|     CZ|    1|\n",
      "|2015-12-12|13:24:37|1236751|    3.2.2|x86_64|mingw32|  RJSONIO|  1.3-0|     DE|    2|\n",
      "|2015-12-12|13:42:35|2077876|    3.2.2|  i386|mingw32|   UsingR|  2.0-5|     CZ|    1|\n",
      "|2015-12-12|13:42:01| 266724|    3.2.2|  i386|mingw32|gridExtra|  2.0.0|     CZ|    1|\n",
      "|2015-12-12|13:00:21|3687766|       NA|    NA|     NA|     lme4| 1.1-10|     DE|    3|\n",
      "|2015-12-12|13:08:56|  57429|       NA|    NA|     NA| testthat| 0.11.0|     DE|    3|\n",
      "|2015-12-12|13:08:09| 216068|    3.2.2|x86_64|mingw32|  mvtnorm|  1.0-3|     DE|    4|\n",
      "|2015-12-12|13:25:00|3595497|    3.2.2|x86_64|mingw32|     maps|  3.0.1|     DE|    2|\n",
      "|2015-12-12|13:25:05|1579597|    3.2.2|x86_64|mingw32|       sp|  1.2-1|     DE|    2|\n",
      "|2015-12-12|13:25:21| 892152|    3.2.3|x86_64|mingw32|geosphere|  1.4-3|     DE|    2|\n",
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT2.show(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'time',\n",
       " 'size',\n",
       " 'r_version',\n",
       " 'r_arch',\n",
       " 'r_os',\n",
       " 'package',\n",
       " 'version',\n",
       " 'country',\n",
       " 'ip_id']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'),\n",
       " ('time', 'string'),\n",
       " ('size', 'string'),\n",
       " ('r_version', 'string'),\n",
       " ('r_arch', 'string'),\n",
       " ('r_os', 'string'),\n",
       " ('package', 'string'),\n",
       " ('version', 'string'),\n",
       " ('country', 'string'),\n",
       " ('ip_id', 'string')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Change DataFrames Properties\n",
    "\n",
    "### 2.1 Change Column Type\n",
    "\n",
    "Availabel types include\n",
    "- BinaryType\n",
    "- BooleanType\n",
    "- ByteType\n",
    "- DoubleType\n",
    "- DateType\n",
    "- FloatType\n",
    "- IntegerType\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .withColumn return a DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "DT3 = DT2.withColumn(\"size\", DT2[\"size\"].cast(IntegerType()))\n",
    "DT3 = DT3.withColumn(\"date\", DT3[\"date\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'date'),\n",
       " ('time', 'string'),\n",
       " ('size', 'int'),\n",
       " ('r_version', 'string'),\n",
       " ('r_arch', 'string'),\n",
       " ('r_os', 'string'),\n",
       " ('package', 'string'),\n",
       " ('version', 'string'),\n",
       " ('country', 'string'),\n",
       " ('ip_id', 'string')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|      date|    time|   size|r_version|r_arch|   r_os|  package|version|country|ip_id|\n",
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "|2015-12-12|13:42:10| 257886|    3.2.2|  i386|mingw32| HistData|  0.7-6|     CZ|    1|\n",
      "|2015-12-12|13:24:37|1236751|    3.2.2|x86_64|mingw32|  RJSONIO|  1.3-0|     DE|    2|\n",
      "|2015-12-12|13:42:35|2077876|    3.2.2|  i386|mingw32|   UsingR|  2.0-5|     CZ|    1|\n",
      "|2015-12-12|13:42:01| 266724|    3.2.2|  i386|mingw32|gridExtra|  2.0.0|     CZ|    1|\n",
      "|2015-12-12|13:00:21|3687766|       NA|    NA|     NA|     lme4| 1.1-10|     DE|    3|\n",
      "+----------+--------+-------+---------+------+-------+---------+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Change Column Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'),\n",
       " ('time', 'string'),\n",
       " ('size_new', 'int'),\n",
       " ('r_version', 'string'),\n",
       " ('r_arch', 'string'),\n",
       " ('r_os', 'string'),\n",
       " ('package', 'string'),\n",
       " ('version', 'string'),\n",
       " ('country', 'string'),\n",
       " ('ip_id', 'int'),\n",
       " ('ip_id_new', 'int')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "# .withColumnRenamed(existing, new) returns a new DataFrame by renaming an existing column.\n",
    "DT4 = DT2\\\n",
    ".withColumn(\"size\", DT2.size.cast(IntegerType()))\\\n",
    ".withColumnRenamed(\"size\", \"size_new\")\\\n",
    ".withColumn(\"ip_id\", DT2.ip_id.cast(IntegerType()))\\\n",
    ".withColumn(\"ip_id_new\", f.col(\"ip_id\")*5)\n",
    "DT4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+---------+------+-------+---------+-------+-------+-----+---------+\n",
      "|      date|    time|size_new|r_version|r_arch|   r_os|  package|version|country|ip_id|ip_id_new|\n",
      "+----------+--------+--------+---------+------+-------+---------+-------+-------+-----+---------+\n",
      "|2015-12-12|13:42:10|  257886|    3.2.2|  i386|mingw32| HistData|  0.7-6|     CZ|    1|        5|\n",
      "|2015-12-12|13:24:37| 1236751|    3.2.2|x86_64|mingw32|  RJSONIO|  1.3-0|     DE|    2|       10|\n",
      "|2015-12-12|13:42:35| 2077876|    3.2.2|  i386|mingw32|   UsingR|  2.0-5|     CZ|    1|        5|\n",
      "|2015-12-12|13:42:01|  266724|    3.2.2|  i386|mingw32|gridExtra|  2.0.0|     CZ|    1|        5|\n",
      "|2015-12-12|13:00:21| 3687766|       NA|    NA|     NA|     lme4| 1.1-10|     DE|    3|       15|\n",
      "+----------+--------+--------+---------+------+-------+---------+-------+-------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Order DataFrame by Specified Column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+---------+------+----+------------+-------+-------+-----+\n",
      "|      date|    time|size|r_version|r_arch|r_os|     package|version|country|ip_id|\n",
      "+----------+--------+----+---------+------+----+------------+-------+-------+-----+\n",
      "|2015-12-12|20:12:20| 504|       NA|    NA|  NA|  KernSmooth| 2.23-4|     CN|13246|\n",
      "|2015-12-12|19:06:56| 504|       NA|    NA|  NA| httpRequest|  0.0.5|     CN| 1133|\n",
      "|2015-12-12|20:33:36| 504|       NA|    NA|  NA|influence.ME|    0.7|     CN| 5337|\n",
      "|2015-12-12|20:34:41| 504|       NA|    NA|  NA|      merror|    1.0|     CN| 5337|\n",
      "|2015-12-12|20:33:02| 504|       NA|    NA|  NA|     granova|    1.4|     CN| 5331|\n",
      "|2015-12-12|20:36:24| 504|       NA|    NA|  NA|       pheno|    1.5|     CN| 4943|\n",
      "|2015-12-12|20:20:28| 504|       NA|    NA|  NA|   orloca.es|    3.2|     CN| 2365|\n",
      "|2015-12-12|20:20:58| 504|       NA|    NA|  NA| poistweedie|    1.0|     CN|   74|\n",
      "|2015-12-12|19:09:58| 504|       NA|    NA|  NA|     polycor|  0.7-0|     CN| 1153|\n",
      "|2015-12-12|20:35:15| 504|       NA|    NA|  NA|      muStat|  1.6.0|     CN| 4943|\n",
      "+----------+--------+----+---------+------+----+------------+-------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT3.sort(DT3.size.asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+---------+------+------------+--------------------+-------+-------+-----+\n",
      "|      date|    time|    size|r_version|r_arch|        r_os|             package|version|country|ip_id|\n",
      "+----------+--------+--------+---------+------+------------+--------------------+-------+-------+-----+\n",
      "|2015-12-12|02:28:49|68736865|    3.3.0|x86_64|   linux-gnu|           TCGA2STAT|    1.2|     US| 2700|\n",
      "|2015-12-12|01:31:52|68736865|    3.1.3|x86_64|darwin10.8.0|           TCGA2STAT|    1.2|     US| 2700|\n",
      "|2015-12-12|02:27:31|68736865|    3.2.3|x86_64|     mingw32|           TCGA2STAT|    1.2|     US| 2700|\n",
      "|2015-12-12|02:28:30|68736865|    3.2.3|x86_64|     mingw32|           TCGA2STAT|    1.2|     US| 2700|\n",
      "|2015-12-12|21:23:23|68736862|    3.2.3|x86_64|darwin13.4.0|           TCGA2STAT|    1.2|     US| 2700|\n",
      "|2015-12-12|02:19:32|68736862|    3.2.0|  i386|     mingw32|           TCGA2STAT|    1.2|     US| 2700|\n",
      "|2015-12-12|17:06:20|68736856|       NA|    NA|          NA|           TCGA2STAT|    1.2|     US| 3084|\n",
      "|2015-12-12|13:17:41|68736856|    3.2.3|x86_64|   linux-gnu|           TCGA2STAT|    1.2|     GB|  548|\n",
      "|2015-12-12|01:28:03|68736856|    3.2.3|x86_64|     mingw32|           TCGA2STAT|    1.2|     US| 2700|\n",
      "|2015-12-12|01:41:08|62559786|       NA|    NA|          NA|ChemometricsWithR...|  0.1.3|     US| 7666|\n",
      "+----------+--------+--------+---------+------+------------+--------------------+-------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT3.sort(DT3.size.desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtering, and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11161009458040756"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT3.filter(DT3['size'] <1000).count() / DT3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009273193054466087"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT3.filter(DT3['package'] == \"ggplot2\").count() / DT3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|     package|count|\n",
      "+------------+-----+\n",
      "|        Rcpp| 4783|\n",
      "|     ggplot2| 3913|\n",
      "|     stringi| 3748|\n",
      "|     stringr| 3449|\n",
      "|        plyr| 3436|\n",
      "|    magrittr| 3265|\n",
      "|      digest| 3223|\n",
      "|    reshape2| 3205|\n",
      "|RColorBrewer| 3046|\n",
      "|      scales| 3007|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT3.groupBy(\"package\").count().sort(\"count\", ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_count = DT3.groupBy(\"package\").count().sort(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|     package|count|\n",
      "+------------+-----+\n",
      "|        Rcpp| 4783|\n",
      "|     ggplot2| 3913|\n",
      "|     stringi| 3748|\n",
      "|     stringr| 3449|\n",
      "|        plyr| 3436|\n",
      "|    magrittr| 3265|\n",
      "|      digest| 3223|\n",
      "|    reshape2| 3205|\n",
      "|RColorBrewer| 3046|\n",
      "|      scales| 3007|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "package_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('date', 'string'), ('time', 'string'), ('size_new', 'int'), ('r_version', 'string'), ('r_arch', 'string'), ('r_os', 'string'), ('package', 'string'), ('version', 'string'), ('country', 'string'), ('ip_id', 'int'), ('ip_id_new', 'int')]\n",
      "+----------+------------------+------------------+------------------+\n",
      "|   package|     avg(size_new)|        avg(ip_id)|    avg(ip_id_new)|\n",
      "+----------+------------------+------------------+------------------+\n",
      "|   RJSONIO| 1177880.798969072| 7003.436671575847|35017.183357879236|\n",
      "|    UsingR|1763649.4503311259| 6566.039735099338|32830.198675496686|\n",
      "|      lme4|         4771306.7| 6236.024358974359|31180.121794871793|\n",
      "|  testthat|149689.25042444823| 5210.857385398981|26054.286926994908|\n",
      "|      maps| 2622275.320302648| 6322.675914249685|31613.379571248424|\n",
      "|      mgcv|1550589.0213980027| 6926.507132667618| 34632.53566333809|\n",
      "|    gtools| 97494.75194300518| 6971.608808290155|34858.044041450776|\n",
      "|matrixcalc|  142329.431372549| 5653.700980392156|28268.504901960783|\n",
      "| lubridate| 538029.1677316293| 6495.372204472844|32476.861022364217|\n",
      "|doParallel|176764.07454819276| 7040.114457831325|35200.572289156626|\n",
      "|     abind| 35663.39405204461|6140.9275092936805|30704.637546468402|\n",
      "|DiagrammeR| 3435750.703703704| 5153.506172839506| 25767.53086419753|\n",
      "| htmltools| 67587.48222222223| 6388.404444444444|31942.022222222222|\n",
      "|   TH.data| 5125837.197368421|5820.3026315789475|29101.513157894737|\n",
      "|      mime|25311.783426741393|6998.1869495596475| 34990.93474779824|\n",
      "|  evaluate|29832.800382043933| 6782.650429799427| 33913.25214899713|\n",
      "|   formatR| 44702.69104991394| 6283.491394148021|31417.456970740102|\n",
      "| XLConnect| 4337568.444700461| 6554.023041474655|32770.115207373274|\n",
      "| rmarkdown| 1215069.107321966| 6233.667001003009|31168.335005015044|\n",
      "|    relimp| 36603.78918918919| 5420.491891891892| 27102.45945945946|\n",
      "+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(DT4.dtypes)\n",
    "DT4.groupBy(\"package\").avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+------------------+---------+------+-----------+-------+-----------------+-------+-----------------+------------------+\n",
      "|summary|      date|    time|          size_new|r_version|r_arch|       r_os|package|          version|country|            ip_id|         ip_id_new|\n",
      "+-------+----------+--------+------------------+---------+------+-----------+-------+-----------------+-------+-----------------+------------------+\n",
      "|  count|    421969|  421969|            421969|   421969|421969|     421969| 421969|           421969| 421969|           421969|            421969|\n",
      "|   mean|      null|    null| 861961.3297967386|     null|  null|       null|   null|63.38614840474673|   null| 5538.41794302425|27692.089715121252|\n",
      "| stddev|      null|    null|2053088.0765684885|     null|  null|       null|   null|418.3377127051209|   null|4462.522749666886| 22312.61374833449|\n",
      "|    min|2015-12-12|00:00:00|               504|   2.11.1|    NA|         NA|     A3|            0-0-1|     A1|                1|                 5|\n",
      "|    max|2015-12-12|23:59:59|          68736865|       NA|x86_64|solaris2.10|    zyp|               NA|     ZM|            16962|             84810|\n",
      "+-------+----------+--------+------------------+---------+------+-----------+-------+-----------------+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT4.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transform A DataFrame Column (using UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------+\n",
      "|     package|count|  perc|\n",
      "+------------+-----+------+\n",
      "|        Rcpp| 4783|1.133%|\n",
      "|     ggplot2| 3913|0.927%|\n",
      "|     stringi| 3748|0.888%|\n",
      "|     stringr| 3449|0.817%|\n",
      "|        plyr| 3436|0.814%|\n",
      "|    magrittr| 3265|0.774%|\n",
      "|      digest| 3223|0.764%|\n",
      "|    reshape2| 3205| 0.76%|\n",
      "|RColorBrewer| 3046|0.722%|\n",
      "|      scales| 3007|0.713%|\n",
      "+------------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "derive_perc = udf(lambda x: str(round(x * 100, 3)) + \"%\")\n",
    "# or \n",
    "# @udf\n",
    "# def derive_perc(x):\n",
    "#     return(str(round(x * 100, 3)) + \"%\")\n",
    "\n",
    "package_count = package_count.withColumn(\"perc\", derive_perc(package_count['count'] / DT3.count()))\n",
    "\n",
    "package_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n",
      "|package|count|  perc|\n",
      "+-------+-----+------+\n",
      "|     DT|   97|0.023%|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "package_count.filter(package_count.package == 'DT').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a view with the Spark DataFrame, then we can SQL syntax to further process our data.\n",
    "\n",
    "You may notice there are two ways to build a view,\n",
    "\n",
    "- `DF.createGlobalTempView` (or `DF.createOrReplaceGlobalTempView`): create a global temporary view\n",
    "- `DF.createTempView` (or `DF.createOrReplaceTempView`): create a local temporary view\n",
    "\n",
    "The main difference between them is the lifetime. The lifetime of a global temporary view is tied to the Spark application, while lifetime of a local temporary view is tied to the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates or replaces a local temporary view with a DataFrame.\n",
    "# The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame.\n",
    "\n",
    "package_count.createOrReplaceTempView(\"package_count_sql_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(perc='0.023%')]\n"
     ]
    }
   ],
   "source": [
    "# Basic Spark SQL Query - 1\n",
    "query_result = sqlContext.sql(\"select perc \\\n",
    "                               from package_count_sql_table \\\n",
    "                               where package = 'DT'\")\n",
    "\n",
    "print(query_result.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n",
      "|package|count|  perc|\n",
      "+-------+-----+------+\n",
      "|   slam| 1006|0.238%|\n",
      "|     sp| 1020|0.242%|\n",
      "|  shiny| 1041|0.247%|\n",
      "|  tidyr| 1042|0.247%|\n",
      "|plotrix| 1066|0.253%|\n",
      "+-------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Basic Spark SQL Query - 2\n",
    "query_result = sqlContext.sql(\"select * \\\n",
    "                                from package_count_sql_table \\\n",
    "                                where count > 1000 \\\n",
    "                                order by count asc\")\n",
    "print(query_result.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slam:0.238%',\n",
       " 'sp:0.242%',\n",
       " 'shiny:0.247%',\n",
       " 'tidyr:0.247%',\n",
       " 'plotrix:0.253%',\n",
       " 'wordcloud:0.254%',\n",
       " 'rgl:0.257%',\n",
       " 'markdown:0.261%',\n",
       " 'irlba:0.27%',\n",
       " 'pkgmaker:0.27%']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Spark RDD way to process the results from Spark SQL query result\n",
    "query_result.rdd.map(lambda x:x['package'] + \":\" + x['perc']).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
